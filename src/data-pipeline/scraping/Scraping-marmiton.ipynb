{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002f1642-9dab-4718-b163-bfab8a6751db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa483bde-b146-4dd5-8ccc-1ff4db213d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ DÃ©marrage du scraping intensif Marmiton...\n",
      "\n",
      "ðŸ“‚ CatÃ©gorie : plat-principal\n",
      "âœ“ Page 1 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 2 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 3 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 4 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 5 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 6 - 21 URLs trouvÃ©es\n",
      "  [1/96] Scraping...\n",
      "  [2/96] Scraping...\n",
      "  [3/96] Scraping...\n",
      "  [4/96] Scraping...\n",
      "  [5/96] Scraping...\n",
      "  [6/96] Scraping...\n",
      "  [7/96] Scraping...\n",
      "  [8/96] Scraping...\n",
      "  [9/96] Scraping...\n",
      "  [10/96] Scraping...\n",
      "  [11/96] Scraping...\n",
      "  [12/96] Scraping...\n",
      "  [13/96] Scraping...\n",
      "  [14/96] Scraping...\n",
      "  [15/96] Scraping...\n",
      "  [16/96] Scraping...\n",
      "  [17/96] Scraping...\n",
      "  [18/96] Scraping...\n",
      "  [19/96] Scraping...\n",
      "  [20/96] Scraping...\n",
      "  [21/96] Scraping...\n",
      "  [22/96] Scraping...\n",
      "  [23/96] Scraping...\n",
      "  [24/96] Scraping...\n",
      "  [25/96] Scraping...\n",
      "  [26/96] Scraping...\n",
      "  [27/96] Scraping...\n",
      "  [28/96] Scraping...\n",
      "  [29/96] Scraping...\n",
      "  [30/96] Scraping...\n",
      "  [31/96] Scraping...\n",
      "  [32/96] Scraping...\n",
      "  [33/96] Scraping...\n",
      "  [34/96] Scraping...\n",
      "  [35/96] Scraping...\n",
      "  [36/96] Scraping...\n",
      "  [37/96] Scraping...\n",
      "  [38/96] Scraping...\n",
      "  [39/96] Scraping...\n",
      "  [40/96] Scraping...\n",
      "  [41/96] Scraping...\n",
      "  [42/96] Scraping...\n",
      "  [43/96] Scraping...\n",
      "  [44/96] Scraping...\n",
      "  [45/96] Scraping...\n",
      "  [46/96] Scraping...\n",
      "  [47/96] Scraping...\n",
      "  [48/96] Scraping...\n",
      "  [49/96] Scraping...\n",
      "  [50/96] Scraping...\n",
      "  [51/96] Scraping...\n",
      "  [52/96] Scraping...\n",
      "  [53/96] Scraping...\n",
      "  [54/96] Scraping...\n",
      "  [55/96] Scraping...\n",
      "  [56/96] Scraping...\n",
      "  [57/96] Scraping...\n",
      "  [58/96] Scraping...\n",
      "  [59/96] Scraping...\n",
      "  [60/96] Scraping...\n",
      "  [61/96] Scraping...\n",
      "  [62/96] Scraping...\n",
      "  [63/96] Scraping...\n",
      "  [64/96] Scraping...\n",
      "  [65/96] Scraping...\n",
      "  [66/96] Scraping...\n",
      "  [67/96] Scraping...\n",
      "  [68/96] Scraping...\n",
      "  [69/96] Scraping...\n",
      "  [70/96] Scraping...\n",
      "  [71/96] Scraping...\n",
      "  [72/96] Scraping...\n",
      "  [73/96] Scraping...\n",
      "  [74/96] Scraping...\n",
      "  [75/96] Scraping...\n",
      "  [76/96] Scraping...\n",
      "  [77/96] Scraping...\n",
      "  [78/96] Scraping...\n",
      "  [79/96] Scraping...\n",
      "  [80/96] Scraping...\n",
      "  [81/96] Scraping...\n",
      "  [82/96] Scraping...\n",
      "  [83/96] Scraping...\n",
      "  [84/96] Scraping...\n",
      "  [85/96] Scraping...\n",
      "  [86/96] Scraping...\n",
      "  [87/96] Scraping...\n",
      "  [88/96] Scraping...\n",
      "  [89/96] Scraping...\n",
      "  [90/96] Scraping...\n",
      "  [91/96] Scraping...\n",
      "  [92/96] Scraping...\n",
      "  [93/96] Scraping...\n",
      "  [94/96] Scraping...\n",
      "  [95/96] Scraping...\n",
      "  [96/96] Scraping...\n",
      "\n",
      "ðŸ“‚ CatÃ©gorie : entree\n",
      "âœ“ Page 1 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 2 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 3 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 4 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 5 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 6 - 21 URLs trouvÃ©es\n",
      "  [1/96] Scraping...\n",
      "  [2/96] Scraping...\n",
      "  [3/96] Scraping...\n",
      "  [4/96] Scraping...\n",
      "  [5/96] Scraping...\n",
      "  [6/96] Scraping...\n",
      "  [7/96] Scraping...\n",
      "  [8/96] Scraping...\n",
      "  [9/96] Scraping...\n",
      "  [10/96] Scraping...\n",
      "  [11/96] Scraping...\n",
      "  [12/96] Scraping...\n",
      "  [13/96] Scraping...\n",
      "  [14/96] Scraping...\n",
      "  [15/96] Scraping...\n",
      "  [16/96] Scraping...\n",
      "  [17/96] Scraping...\n",
      "  [18/96] Scraping...\n",
      "  [19/96] Scraping...\n",
      "  [20/96] Scraping...\n",
      "  [21/96] Scraping...\n",
      "  [22/96] Scraping...\n",
      "  [23/96] Scraping...\n",
      "  [24/96] Scraping...\n",
      "  [25/96] Scraping...\n",
      "  [26/96] Scraping...\n",
      "  [27/96] Scraping...\n",
      "  [28/96] Scraping...\n",
      "  [29/96] Scraping...\n",
      "  [30/96] Scraping...\n",
      "  [31/96] Scraping...\n",
      "  [32/96] Scraping...\n",
      "  [33/96] Scraping...\n",
      "  [34/96] Scraping...\n",
      "  [35/96] Scraping...\n",
      "  [36/96] Scraping...\n",
      "  [37/96] Scraping...\n",
      "  [38/96] Scraping...\n",
      "  [39/96] Scraping...\n",
      "  [40/96] Scraping...\n",
      "  [41/96] Scraping...\n",
      "  [42/96] Scraping...\n",
      "  [43/96] Scraping...\n",
      "  [44/96] Scraping...\n",
      "  [45/96] Scraping...\n",
      "  [46/96] Scraping...\n",
      "  [47/96] Scraping...\n",
      "  [48/96] Scraping...\n",
      "  [49/96] Scraping...\n",
      "  [50/96] Scraping...\n",
      "  [51/96] Scraping...\n",
      "  [52/96] Scraping...\n",
      "  [53/96] Scraping...\n",
      "  [54/96] Scraping...\n",
      "  [55/96] Scraping...\n",
      "  [56/96] Scraping...\n",
      "  [57/96] Scraping...\n",
      "  [58/96] Scraping...\n",
      "  [59/96] Scraping...\n",
      "  [60/96] Scraping...\n",
      "  [61/96] Scraping...\n",
      "  [62/96] Scraping...\n",
      "  [63/96] Scraping...\n",
      "  [64/96] Scraping...\n",
      "  [65/96] Scraping...\n",
      "  [66/96] Scraping...\n",
      "  [67/96] Scraping...\n",
      "  [68/96] Scraping...\n",
      "  [69/96] Scraping...\n",
      "  [70/96] Scraping...\n",
      "  [71/96] Scraping...\n",
      "  [72/96] Scraping...\n",
      "  [73/96] Scraping...\n",
      "  [74/96] Scraping...\n",
      "  [75/96] Scraping...\n",
      "  [76/96] Scraping...\n",
      "  [77/96] Scraping...\n",
      "  [78/96] Scraping...\n",
      "  [79/96] Scraping...\n",
      "  [80/96] Scraping...\n",
      "  [81/96] Scraping...\n",
      "  [82/96] Scraping...\n",
      "  [83/96] Scraping...\n",
      "  [84/96] Scraping...\n",
      "  [85/96] Scraping...\n",
      "  [86/96] Scraping...\n",
      "  [87/96] Scraping...\n",
      "  [88/96] Scraping...\n",
      "  [89/96] Scraping...\n",
      "  [90/96] Scraping...\n",
      "  [91/96] Scraping...\n",
      "  [92/96] Scraping...\n",
      "  [93/96] Scraping...\n",
      "  [94/96] Scraping...\n",
      "  [95/96] Scraping...\n",
      "  [96/96] Scraping...\n",
      "\n",
      "ðŸ“‚ CatÃ©gorie : dessert\n",
      "âœ“ Page 1 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 2 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 3 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 4 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 5 - 21 URLs trouvÃ©es\n",
      "âœ“ Page 6 - 21 URLs trouvÃ©es\n",
      "  [1/96] Scraping...\n",
      "  [2/96] Scraping...\n",
      "  [3/96] Scraping...\n",
      "  [4/96] Scraping...\n",
      "  [5/96] Scraping...\n",
      "  [6/96] Scraping...\n",
      "  [7/96] Scraping...\n",
      "  [8/96] Scraping...\n",
      "  [9/96] Scraping...\n",
      "  [10/96] Scraping...\n",
      "  [11/96] Scraping...\n",
      "  [12/96] Scraping...\n",
      "  [13/96] Scraping...\n",
      "  [14/96] Scraping...\n",
      "  [15/96] Scraping...\n",
      "  [16/96] Scraping...\n",
      "  [17/96] Scraping...\n",
      "  [18/96] Scraping...\n",
      "  [19/96] Scraping...\n",
      "  [20/96] Scraping...\n",
      "  [21/96] Scraping...\n",
      "  [22/96] Scraping...\n",
      "  [23/96] Scraping...\n",
      "  [24/96] Scraping...\n",
      "  [25/96] Scraping...\n",
      "  [26/96] Scraping...\n",
      "  [27/96] Scraping...\n",
      "  [28/96] Scraping...\n",
      "  [29/96] Scraping...\n",
      "  [30/96] Scraping...\n",
      "  [31/96] Scraping...\n",
      "  [32/96] Scraping...\n",
      "  [33/96] Scraping...\n",
      "  [34/96] Scraping...\n",
      "  [35/96] Scraping...\n",
      "  [36/96] Scraping...\n",
      "  [37/96] Scraping...\n",
      "  [38/96] Scraping...\n",
      "  [39/96] Scraping...\n",
      "  [40/96] Scraping...\n",
      "  [41/96] Scraping...\n",
      "  [42/96] Scraping...\n",
      "  [43/96] Scraping...\n",
      "  [44/96] Scraping...\n",
      "  [45/96] Scraping...\n",
      "  [46/96] Scraping...\n",
      "  [47/96] Scraping...\n",
      "  [48/96] Scraping...\n",
      "  [49/96] Scraping...\n",
      "  [50/96] Scraping...\n",
      "  [51/96] Scraping...\n",
      "  [52/96] Scraping...\n",
      "  [53/96] Scraping...\n",
      "  [54/96] Scraping...\n",
      "  [55/96] Scraping...\n",
      "  [56/96] Scraping...\n",
      "  [57/96] Scraping...\n",
      "  [58/96] Scraping...\n",
      "  [59/96] Scraping...\n",
      "  [60/96] Scraping...\n",
      "  [61/96] Scraping...\n",
      "  [62/96] Scraping...\n",
      "  [63/96] Scraping...\n",
      "  [64/96] Scraping...\n",
      "  [65/96] Scraping...\n",
      "  [66/96] Scraping...\n",
      "  [67/96] Scraping...\n",
      "  [68/96] Scraping...\n",
      "  [69/96] Scraping...\n",
      "  [70/96] Scraping...\n",
      "  [71/96] Scraping...\n",
      "  [72/96] Scraping...\n",
      "  [73/96] Scraping...\n",
      "  [74/96] Scraping...\n",
      "  [75/96] Scraping...\n",
      "  [76/96] Scraping...\n",
      "  [77/96] Scraping...\n",
      "  [78/96] Scraping...\n",
      "  [79/96] Scraping...\n",
      "  [80/96] Scraping...\n",
      "  [81/96] Scraping...\n",
      "  [82/96] Scraping...\n",
      "  [83/96] Scraping...\n",
      "  [84/96] Scraping...\n",
      "  [85/96] Scraping...\n",
      "  [86/96] Scraping...\n",
      "  [87/96] Scraping...\n",
      "  [88/96] Scraping...\n",
      "  [89/96] Scraping...\n",
      "  [90/96] Scraping...\n",
      "  [91/96] Scraping...\n",
      "  [92/96] Scraping...\n",
      "  [93/96] Scraping...\n",
      "  [94/96] Scraping...\n",
      "  [95/96] Scraping...\n",
      "  [96/96] Scraping...\n",
      "\n",
      "âœ… 288 recettes sauvegardÃ©es dans marmiton_v2.csv\n",
      "\n",
      "ðŸ“Š Statistiques finales :\n",
      "  - Total recettes collectÃ©es : 288\n",
      "  âœ… Objectif atteint : 288 recettes prÃªtes pour l'importation.\n",
      "  - RÃ©partition : {'plat': 148, 'dessert': 103, 'entree': 37}\n"
     ]
    }
   ],
   "source": [
    "class MarmitonScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.marmiton.org\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "    def get_recipe_urls(self, category=\"plat-principal\", max_pages=3):\n",
    "        urls = []\n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                search_url = f\"{self.base_url}/recettes/recherche.aspx?aqt={category}&page={page}\"\n",
    "                response = self.session.get(search_url, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                links = soup.find_all('a', href=re.compile(r'/recettes/recette_'))\n",
    "                \n",
    "                page_urls = [urljoin(self.base_url, link.get('href')) for link in links if link.get('href')]\n",
    "                urls.extend(page_urls)\n",
    "                print(f\"âœ“ Page {page} - {len(page_urls)} URLs trouvÃ©es\")\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Erreur page {page}: {e}\")\n",
    "        return list(set(urls))\n",
    "\n",
    "    def extract_recipe_details(self, url):\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extraction des badges (Temps, DifficultÃ©, Budget)\n",
    "            badges = [b.text.strip().upper() for b in soup.select('.recipe-primary__item')]\n",
    "            \n",
    "            # Initialisation des variables\n",
    "            diff = \"MOYEN\"\n",
    "            budget = \"NON SPÃ‰CIFIÃ‰\"\n",
    "            t_prep = 0\n",
    "            t_cuis = 0\n",
    "\n",
    "            # Attribution intelligente (au lieu des index fixes)\n",
    "            for b in badges:\n",
    "                if \"MIN\" in b or \"H\" in b:\n",
    "                    if \"PRÃ‰P\" in b.lower(): t_prep = self._parse_minutes(b)\n",
    "                    elif \"CUIS\" in b.lower(): t_cuis = self._parse_minutes(b)\n",
    "                    elif t_prep == 0: t_prep = self._parse_minutes(b) # Fallback\n",
    "                if any(x in b for x in [\"FACILE\", \"MOYEN\", \"DIFFICILE\"]):\n",
    "                    diff = b.replace(\"TRÃˆS \", \"\")\n",
    "                if any(x in b for x in [\"BON MARCHÃ‰\", \"COÃ›TEUX\"]):\n",
    "                    budget = b\n",
    "\n",
    "            title = self._extract_title(soup)\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'titre': title,\n",
    "                'description': self._extract_description(soup),\n",
    "                'difficulte': diff,\n",
    "                'budget': budget,\n",
    "                'temps_preparation': t_prep,\n",
    "                'temps_cuisson': t_cuis,\n",
    "                'type_recette': self._classify_type(title),\n",
    "                'cuisine': self._classify_cuisine(title),\n",
    "                'ingredients': self._extract_ingredients(soup), # Garder en liste\n",
    "                'etapes': self._extract_steps(soup),           # Garder en liste\n",
    "                'date_scraping': datetime.now().isoformat(),\n",
    "                'imageUrl': self._extract_image(soup)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sur {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_minutes(self, text):\n",
    "        \"\"\"Convertit '1h20' ou '20 min' en minutes (int)\"\"\"\n",
    "        text = text.upper()\n",
    "        digits = re.findall(r'\\d+', text)\n",
    "        if not digits: return 0\n",
    "        if 'H' in text:\n",
    "            hours = int(digits[0])\n",
    "            mins = int(digits[1]) if len(digits) > 1 else 0\n",
    "            return (hours * 60) + mins\n",
    "        return int(digits[0])\n",
    "\n",
    "    # --- NOUVELLES MÃ‰THODES D'EXTRACTION ---\n",
    "\n",
    "    def _extract_description(self, soup):\n",
    "        # Cherche la meta description ou le texte d'introduction\n",
    "        desc = soup.find('meta', property='og:description')\n",
    "        if desc: return desc['content'].strip()\n",
    "        # Fallback sur le texte de prÃ©sentation si dispo\n",
    "        summary = soup.select_one('.recipe-content__description')\n",
    "        return summary.text.strip() if summary else \"\"\n",
    "\n",
    "    def _extract_badge_info(self, soup, index):\n",
    "        # Marmiton utilise des classes type \"recipe-primary__item\" pour difficultÃ©/budget\n",
    "        badges = soup.select('.recipe-primary__item')\n",
    "        try:\n",
    "            if badges and len(badges) > index:\n",
    "                return badges[index].text.strip().upper()\n",
    "        except:\n",
    "            pass\n",
    "        return \"NON SPÃ‰CIFIÃ‰\"\n",
    "\n",
    "    def _extract_time(self, soup, type_time):\n",
    "        # Cherche dans les Ã©lÃ©ments de temps (ex: \"15 min\")\n",
    "        time_elements = soup.select('.recipe-primary__item')\n",
    "        for el in time_elements:\n",
    "            text = el.text.lower()\n",
    "            if type_time in text:\n",
    "                # Extraction du nombre via regex\n",
    "                numbers = re.findall(r'\\d+', text)\n",
    "                return int(numbers[0]) if numbers else 0\n",
    "        return 0\n",
    "\n",
    "    def _extract_image(self, soup):\n",
    "        img = soup.find('meta', property='og:image')\n",
    "        return img['content'] if img else \"\"\n",
    "\n",
    "    def _extract_title(self, soup):\n",
    "        title = soup.find('h1')\n",
    "        return title.text.strip() if title else \"Sans titre\"\n",
    "\n",
    "    def _classify_type(self, title):\n",
    "        title = title.lower()\n",
    "        if any(x in title for x in ['tarte', 'gÃ¢teau', 'mousse', 'dessert']): return 'dessert'\n",
    "        if any(x in title for x in ['entrÃ©e', 'salade', 'soupe']): return 'entree'\n",
    "        return 'plat'\n",
    "\n",
    "    def _classify_cuisine(self, title):\n",
    "        title = title.lower()\n",
    "        if 'pizza' in title or 'pasta' in title: return 'italienne'\n",
    "        return 'franÃ§aise'\n",
    "\n",
    "    def _extract_ingredients(self, soup):\n",
    "        ingredients = []\n",
    "        # SÃ©lecteur spÃ©cifique Ã  la nouvelle version de Marmiton\n",
    "        items = soup.select('.card-ingredient')\n",
    "        for item in items:\n",
    "            name = item.select_one('.ingredient-name')\n",
    "            if name: ingredients.append({\"nom\": name.text.strip()})\n",
    "        return ingredients\n",
    "\n",
    "    def _extract_steps(self, soup):\n",
    "        items = soup.select('div[class*=\"RecipeStep\"]')\n",
    "        return [item.get_text().strip() for item in items]\n",
    "\n",
    "    def scrape_all(self, categories=['plat-principal'], max_pages=1):\n",
    "        all_results = []\n",
    "        for cat in categories:\n",
    "            print(f\"\\nðŸ“‚ CatÃ©gorie : {cat}\")\n",
    "            urls = self.get_recipe_urls(cat, max_pages)\n",
    "            for i, url in enumerate(urls, 1):\n",
    "                print(f\"  [{i}/{len(urls)}] Scraping...\")\n",
    "                data = self.extract_recipe_details(url)\n",
    "                if data: all_results.append(data)\n",
    "                time.sleep(random.uniform(0.5, 1.5))\n",
    "        return all_results\n",
    "\n",
    "    def save_to_csv(self, recipes, filename='marmiton_v2.csv'):\n",
    "        if not recipes:\n",
    "            print(\"âš  Aucune donnÃ©e collectÃ©e. Le fichier n'a pas Ã©tÃ© crÃ©Ã©.\")\n",
    "            return pd.DataFrame() # On renvoie un DataFrame vide plutÃ´t que None\n",
    "        \n",
    "        df = pd.DataFrame(recipes)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nâœ… {len(df)} recettes sauvegardÃ©es dans {filename}\")\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = MarmitonScraper()\n",
    "    print(\"ðŸš€ DÃ©marrage du scraping intensif Marmiton...\")\n",
    "    \n",
    "    # MODIFICATION ICI :\n",
    "    # 3 catÃ©gories x 4 pages (~15 recettes/page) = ~180 URLs potentielles\n",
    "    # On vise large car certaines recettes peuvent Ã©chouer ou Ãªtre des doublons\n",
    "    recipes = scraper.scrape_all(\n",
    "        categories=['plat-principal', 'entree', 'dessert'],\n",
    "        max_pages=6  # Augmentez Ã  5 ou 6 si vous voulez Ãªtre sÃ»r d'avoir 140+\n",
    "    )\n",
    "    \n",
    "    # Sauvegarde\n",
    "    df = scraper.save_to_csv(recipes, filename='marmiton_v2.csv')\n",
    "    \n",
    "    # VÃ©rification et statistiques\n",
    "    if df is not None and not df.empty:\n",
    "        print(f\"\\nðŸ“Š Statistiques finales :\")\n",
    "        print(f\"  - Total recettes collectÃ©es : {len(df)}\")\n",
    "        \n",
    "        if len(df) < 140:\n",
    "            print(f\"  âš ï¸ Attention : Vous n'avez que {len(df)} recettes. Relancez avec plus de max_pages.\")\n",
    "        else:\n",
    "            print(f\"  âœ… Objectif atteint : {len(df)} recettes prÃªtes pour l'importation.\")\n",
    "            \n",
    "        if 'type_recette' in df.columns:\n",
    "            print(f\"  - RÃ©partition : {df['type_recette'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Ã‰chec du scraping : aucune donnÃ©e rÃ©cupÃ©rÃ©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4767e-e178-4c0c-8864-11f976d987d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d5d96-25c6-43b7-aa2a-6e5e5a90feea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

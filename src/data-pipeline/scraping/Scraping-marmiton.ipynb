{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002f1642-9dab-4718-b163-bfab8a6751db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa483bde-b146-4dd5-8ccc-1ff4db213d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage du scraping Marmiton...\n",
      "\n",
      "üìÇ Cat√©gorie : plat-principal\n",
      "‚úì Page 1 - 21 URLs trouv√©es\n",
      "‚úì Page 2 - 21 URLs trouv√©es\n",
      "‚úì Page 3 - 21 URLs trouv√©es\n",
      "‚úì Page 4 - 21 URLs trouv√©es\n",
      "‚úì Page 5 - 21 URLs trouv√©es\n",
      "‚úì Page 6 - 21 URLs trouv√©es\n",
      "‚úì Page 7 - 21 URLs trouv√©es\n",
      "‚úì Page 8 - 21 URLs trouv√©es\n",
      "‚úì Page 9 - 21 URLs trouv√©es\n",
      "‚úì Page 10 - 21 URLs trouv√©es\n",
      "  [1/156] Scraping...\n",
      "  [2/156] Scraping...\n",
      "  [3/156] Scraping...\n",
      "  [4/156] Scraping...\n",
      "  [5/156] Scraping...\n",
      "  [6/156] Scraping...\n",
      "  [7/156] Scraping...\n",
      "  [8/156] Scraping...\n",
      "  [9/156] Scraping...\n",
      "  [10/156] Scraping...\n",
      "  [11/156] Scraping...\n",
      "  [12/156] Scraping...\n",
      "  [13/156] Scraping...\n",
      "  [14/156] Scraping...\n",
      "  [15/156] Scraping...\n",
      "  [16/156] Scraping...\n",
      "  [17/156] Scraping...\n",
      "  [18/156] Scraping...\n",
      "  [19/156] Scraping...\n",
      "  [20/156] Scraping...\n",
      "  [21/156] Scraping...\n",
      "  [22/156] Scraping...\n",
      "  [23/156] Scraping...\n",
      "  [24/156] Scraping...\n",
      "  [25/156] Scraping...\n",
      "  [26/156] Scraping...\n",
      "  [27/156] Scraping...\n",
      "  [28/156] Scraping...\n",
      "  [29/156] Scraping...\n",
      "  [30/156] Scraping...\n",
      "  [31/156] Scraping...\n",
      "  [32/156] Scraping...\n",
      "  [33/156] Scraping...\n",
      "  [34/156] Scraping...\n",
      "  [35/156] Scraping...\n",
      "  [36/156] Scraping...\n",
      "  [37/156] Scraping...\n",
      "  [38/156] Scraping...\n",
      "  [39/156] Scraping...\n",
      "  [40/156] Scraping...\n",
      "  [41/156] Scraping...\n",
      "  [42/156] Scraping...\n",
      "  [43/156] Scraping...\n",
      "  [44/156] Scraping...\n",
      "  [45/156] Scraping...\n",
      "  [46/156] Scraping...\n",
      "  [47/156] Scraping...\n",
      "  [48/156] Scraping...\n",
      "  [49/156] Scraping...\n",
      "  [50/156] Scraping...\n",
      "  [51/156] Scraping...\n",
      "  [52/156] Scraping...\n",
      "  [53/156] Scraping...\n",
      "  [54/156] Scraping...\n",
      "  [55/156] Scraping...\n",
      "  [56/156] Scraping...\n",
      "  [57/156] Scraping...\n",
      "  [58/156] Scraping...\n",
      "  [59/156] Scraping...\n",
      "  [60/156] Scraping...\n",
      "  [61/156] Scraping...\n",
      "  [62/156] Scraping...\n",
      "  [63/156] Scraping...\n",
      "  [64/156] Scraping...\n",
      "  [65/156] Scraping...\n",
      "  [66/156] Scraping...\n",
      "  [67/156] Scraping...\n",
      "  [68/156] Scraping...\n",
      "  [69/156] Scraping...\n",
      "  [70/156] Scraping...\n",
      "  [71/156] Scraping...\n",
      "  [72/156] Scraping...\n",
      "  [73/156] Scraping...\n",
      "  [74/156] Scraping...\n",
      "  [75/156] Scraping...\n",
      "  [76/156] Scraping...\n",
      "  [77/156] Scraping...\n",
      "  [78/156] Scraping...\n",
      "  [79/156] Scraping...\n",
      "  [80/156] Scraping...\n",
      "  [81/156] Scraping...\n",
      "  [82/156] Scraping...\n",
      "  [83/156] Scraping...\n",
      "  [84/156] Scraping...\n",
      "  [85/156] Scraping...\n",
      "  [86/156] Scraping...\n",
      "  [87/156] Scraping...\n",
      "  [88/156] Scraping...\n",
      "  [89/156] Scraping...\n",
      "  [90/156] Scraping...\n",
      "  [91/156] Scraping...\n",
      "  [92/156] Scraping...\n",
      "  [93/156] Scraping...\n",
      "  [94/156] Scraping...\n",
      "  [95/156] Scraping...\n",
      "  [96/156] Scraping...\n",
      "  [97/156] Scraping...\n",
      "  [98/156] Scraping...\n",
      "  [99/156] Scraping...\n",
      "  [100/156] Scraping...\n",
      "  [101/156] Scraping...\n",
      "  [102/156] Scraping...\n",
      "  [103/156] Scraping...\n",
      "  [104/156] Scraping...\n",
      "  [105/156] Scraping...\n",
      "  [106/156] Scraping...\n",
      "  [107/156] Scraping...\n",
      "  [108/156] Scraping...\n",
      "  [109/156] Scraping...\n",
      "  [110/156] Scraping...\n",
      "  [111/156] Scraping...\n",
      "  [112/156] Scraping...\n",
      "  [113/156] Scraping...\n",
      "  [114/156] Scraping...\n",
      "  [115/156] Scraping...\n",
      "  [116/156] Scraping...\n",
      "  [117/156] Scraping...\n",
      "  [118/156] Scraping...\n",
      "  [119/156] Scraping...\n",
      "  [120/156] Scraping...\n",
      "  [121/156] Scraping...\n",
      "  [122/156] Scraping...\n",
      "  [123/156] Scraping...\n",
      "  [124/156] Scraping...\n",
      "  [125/156] Scraping...\n",
      "  [126/156] Scraping...\n",
      "  [127/156] Scraping...\n",
      "  [128/156] Scraping...\n",
      "  [129/156] Scraping...\n",
      "  [130/156] Scraping...\n",
      "  [131/156] Scraping...\n",
      "  [132/156] Scraping...\n",
      "  [133/156] Scraping...\n",
      "  [134/156] Scraping...\n",
      "  [135/156] Scraping...\n",
      "  [136/156] Scraping...\n",
      "  [137/156] Scraping...\n",
      "  [138/156] Scraping...\n",
      "  [139/156] Scraping...\n",
      "  [140/156] Scraping...\n",
      "  [141/156] Scraping...\n",
      "  [142/156] Scraping...\n",
      "  [143/156] Scraping...\n",
      "  [144/156] Scraping...\n",
      "  [145/156] Scraping...\n",
      "  [146/156] Scraping...\n",
      "  [147/156] Scraping...\n",
      "  [148/156] Scraping...\n",
      "  [149/156] Scraping...\n",
      "  [150/156] Scraping...\n",
      "  [151/156] Scraping...\n",
      "  [152/156] Scraping...\n",
      "  [153/156] Scraping...\n",
      "  [154/156] Scraping...\n",
      "  [155/156] Scraping...\n",
      "  [156/156] Scraping...\n",
      "\n",
      "üìÇ Cat√©gorie : entree\n",
      "‚úì Page 1 - 21 URLs trouv√©es\n",
      "‚úì Page 2 - 21 URLs trouv√©es\n",
      "‚úì Page 3 - 21 URLs trouv√©es\n",
      "‚úì Page 4 - 21 URLs trouv√©es\n",
      "‚úì Page 5 - 21 URLs trouv√©es\n",
      "‚úì Page 6 - 21 URLs trouv√©es\n",
      "‚úì Page 7 - 21 URLs trouv√©es\n",
      "‚úì Page 8 - 21 URLs trouv√©es\n",
      "‚úì Page 9 - 21 URLs trouv√©es\n",
      "‚úì Page 10 - 21 URLs trouv√©es\n",
      "  [1/156] Scraping...\n",
      "  [2/156] Scraping...\n",
      "  [3/156] Scraping...\n",
      "  [4/156] Scraping...\n",
      "  [5/156] Scraping...\n",
      "  [6/156] Scraping...\n",
      "  [7/156] Scraping...\n",
      "  [8/156] Scraping...\n",
      "  [9/156] Scraping...\n",
      "  [10/156] Scraping...\n",
      "  [11/156] Scraping...\n",
      "  [12/156] Scraping...\n",
      "  [13/156] Scraping...\n",
      "  [14/156] Scraping...\n",
      "  [15/156] Scraping...\n",
      "  [16/156] Scraping...\n",
      "  [17/156] Scraping...\n",
      "  [18/156] Scraping...\n",
      "  [19/156] Scraping...\n",
      "  [20/156] Scraping...\n",
      "  [21/156] Scraping...\n",
      "  [22/156] Scraping...\n",
      "  [23/156] Scraping...\n",
      "  [24/156] Scraping...\n",
      "  [25/156] Scraping...\n",
      "  [26/156] Scraping...\n",
      "  [27/156] Scraping...\n",
      "  [28/156] Scraping...\n",
      "  [29/156] Scraping...\n",
      "  [30/156] Scraping...\n",
      "  [31/156] Scraping...\n",
      "  [32/156] Scraping...\n",
      "  [33/156] Scraping...\n",
      "  [34/156] Scraping...\n",
      "  [35/156] Scraping...\n",
      "  [36/156] Scraping...\n",
      "  [37/156] Scraping...\n",
      "  [38/156] Scraping...\n",
      "  [39/156] Scraping...\n",
      "  [40/156] Scraping...\n",
      "  [41/156] Scraping...\n",
      "  [42/156] Scraping...\n",
      "  [43/156] Scraping...\n",
      "  [44/156] Scraping...\n",
      "  [45/156] Scraping...\n",
      "  [46/156] Scraping...\n",
      "  [47/156] Scraping...\n",
      "  [48/156] Scraping...\n",
      "  [49/156] Scraping...\n",
      "  [50/156] Scraping...\n",
      "  [51/156] Scraping...\n",
      "  [52/156] Scraping...\n",
      "  [53/156] Scraping...\n",
      "  [54/156] Scraping...\n",
      "  [55/156] Scraping...\n",
      "  [56/156] Scraping...\n",
      "  [57/156] Scraping...\n",
      "  [58/156] Scraping...\n",
      "  [59/156] Scraping...\n",
      "  [60/156] Scraping...\n",
      "  [61/156] Scraping...\n",
      "  [62/156] Scraping...\n",
      "  [63/156] Scraping...\n",
      "  [64/156] Scraping...\n",
      "  [65/156] Scraping...\n",
      "  [66/156] Scraping...\n",
      "  [67/156] Scraping...\n",
      "  [68/156] Scraping...\n",
      "  [69/156] Scraping...\n",
      "  [70/156] Scraping...\n",
      "  [71/156] Scraping...\n",
      "  [72/156] Scraping...\n",
      "  [73/156] Scraping...\n",
      "  [74/156] Scraping...\n",
      "  [75/156] Scraping...\n",
      "  [76/156] Scraping...\n",
      "  [77/156] Scraping...\n",
      "  [78/156] Scraping...\n",
      "  [79/156] Scraping...\n",
      "  [80/156] Scraping...\n",
      "  [81/156] Scraping...\n",
      "  [82/156] Scraping...\n",
      "  [83/156] Scraping...\n",
      "  [84/156] Scraping...\n",
      "  [85/156] Scraping...\n",
      "  [86/156] Scraping...\n",
      "  [87/156] Scraping...\n",
      "  [88/156] Scraping...\n",
      "  [89/156] Scraping...\n",
      "  [90/156] Scraping...\n",
      "  [91/156] Scraping...\n",
      "  [92/156] Scraping...\n",
      "  [93/156] Scraping...\n",
      "  [94/156] Scraping...\n",
      "  [95/156] Scraping...\n",
      "  [96/156] Scraping...\n",
      "  [97/156] Scraping...\n",
      "  [98/156] Scraping...\n",
      "  [99/156] Scraping...\n",
      "  [100/156] Scraping...\n",
      "  [101/156] Scraping...\n",
      "  [102/156] Scraping...\n",
      "  [103/156] Scraping...\n",
      "  [104/156] Scraping...\n",
      "  [105/156] Scraping...\n",
      "  [106/156] Scraping...\n",
      "  [107/156] Scraping...\n",
      "  [108/156] Scraping...\n",
      "  [109/156] Scraping...\n",
      "  [110/156] Scraping...\n",
      "  [111/156] Scraping...\n",
      "  [112/156] Scraping...\n",
      "  [113/156] Scraping...\n",
      "  [114/156] Scraping...\n",
      "  [115/156] Scraping...\n",
      "  [116/156] Scraping...\n",
      "  [117/156] Scraping...\n",
      "  [118/156] Scraping...\n",
      "  [119/156] Scraping...\n",
      "  [120/156] Scraping...\n",
      "  [121/156] Scraping...\n",
      "  [122/156] Scraping...\n",
      "  [123/156] Scraping...\n",
      "  [124/156] Scraping...\n",
      "  [125/156] Scraping...\n",
      "  [126/156] Scraping...\n",
      "  [127/156] Scraping...\n",
      "  [128/156] Scraping...\n",
      "  [129/156] Scraping...\n",
      "  [130/156] Scraping...\n",
      "  [131/156] Scraping...\n",
      "  [132/156] Scraping...\n",
      "  [133/156] Scraping...\n",
      "  [134/156] Scraping...\n",
      "  [135/156] Scraping...\n",
      "  [136/156] Scraping...\n",
      "  [137/156] Scraping...\n",
      "  [138/156] Scraping...\n",
      "  [139/156] Scraping...\n",
      "  [140/156] Scraping...\n",
      "  [141/156] Scraping...\n",
      "  [142/156] Scraping...\n",
      "  [143/156] Scraping...\n",
      "  [144/156] Scraping...\n",
      "  [145/156] Scraping...\n",
      "  [146/156] Scraping...\n",
      "  [147/156] Scraping...\n",
      "  [148/156] Scraping...\n",
      "  [149/156] Scraping...\n",
      "  [150/156] Scraping...\n",
      "  [151/156] Scraping...\n",
      "  [152/156] Scraping...\n",
      "  [153/156] Scraping...\n",
      "  [154/156] Scraping...\n",
      "  [155/156] Scraping...\n",
      "  [156/156] Scraping...\n",
      "\n",
      "üìÇ Cat√©gorie : dessert\n",
      "‚úì Page 1 - 21 URLs trouv√©es\n",
      "‚úì Page 2 - 21 URLs trouv√©es\n",
      "‚úì Page 3 - 21 URLs trouv√©es\n",
      "‚úì Page 4 - 21 URLs trouv√©es\n",
      "‚úì Page 5 - 21 URLs trouv√©es\n",
      "‚úì Page 6 - 21 URLs trouv√©es\n",
      "‚úì Page 7 - 21 URLs trouv√©es\n",
      "‚úì Page 8 - 21 URLs trouv√©es\n",
      "‚úì Page 9 - 21 URLs trouv√©es\n",
      "‚úì Page 10 - 21 URLs trouv√©es\n",
      "  [1/156] Scraping...\n",
      "  [2/156] Scraping...\n",
      "  [3/156] Scraping...\n",
      "  [4/156] Scraping...\n",
      "  [5/156] Scraping...\n",
      "  [6/156] Scraping...\n",
      "  [7/156] Scraping...\n",
      "  [8/156] Scraping...\n",
      "  [9/156] Scraping...\n",
      "  [10/156] Scraping...\n",
      "  [11/156] Scraping...\n",
      "  [12/156] Scraping...\n",
      "  [13/156] Scraping...\n",
      "  [14/156] Scraping...\n",
      "  [15/156] Scraping...\n",
      "  [16/156] Scraping...\n",
      "  [17/156] Scraping...\n",
      "  [18/156] Scraping...\n",
      "  [19/156] Scraping...\n",
      "  [20/156] Scraping...\n",
      "  [21/156] Scraping...\n",
      "  [22/156] Scraping...\n",
      "  [23/156] Scraping...\n",
      "  [24/156] Scraping...\n",
      "  [25/156] Scraping...\n",
      "  [26/156] Scraping...\n",
      "  [27/156] Scraping...\n",
      "  [28/156] Scraping...\n",
      "  [29/156] Scraping...\n",
      "  [30/156] Scraping...\n",
      "  [31/156] Scraping...\n",
      "  [32/156] Scraping...\n",
      "  [33/156] Scraping...\n",
      "  [34/156] Scraping...\n",
      "  [35/156] Scraping...\n",
      "  [36/156] Scraping...\n",
      "  [37/156] Scraping...\n",
      "  [38/156] Scraping...\n",
      "  [39/156] Scraping...\n",
      "  [40/156] Scraping...\n",
      "  [41/156] Scraping...\n",
      "  [42/156] Scraping...\n",
      "  [43/156] Scraping...\n",
      "  [44/156] Scraping...\n",
      "  [45/156] Scraping...\n",
      "  [46/156] Scraping...\n",
      "  [47/156] Scraping...\n",
      "  [48/156] Scraping...\n",
      "  [49/156] Scraping...\n",
      "  [50/156] Scraping...\n",
      "  [51/156] Scraping...\n",
      "  [52/156] Scraping...\n",
      "  [53/156] Scraping...\n",
      "  [54/156] Scraping...\n",
      "  [55/156] Scraping...\n",
      "  [56/156] Scraping...\n",
      "  [57/156] Scraping...\n",
      "  [58/156] Scraping...\n",
      "  [59/156] Scraping...\n",
      "  [60/156] Scraping...\n",
      "  [61/156] Scraping...\n",
      "  [62/156] Scraping...\n",
      "  [63/156] Scraping...\n",
      "  [64/156] Scraping...\n",
      "  [65/156] Scraping...\n",
      "  [66/156] Scraping...\n",
      "  [67/156] Scraping...\n",
      "  [68/156] Scraping...\n",
      "  [69/156] Scraping...\n",
      "  [70/156] Scraping...\n",
      "  [71/156] Scraping...\n",
      "  [72/156] Scraping...\n",
      "  [73/156] Scraping...\n",
      "  [74/156] Scraping...\n",
      "  [75/156] Scraping...\n",
      "  [76/156] Scraping...\n",
      "  [77/156] Scraping...\n",
      "  [78/156] Scraping...\n",
      "  [79/156] Scraping...\n",
      "  [80/156] Scraping...\n",
      "  [81/156] Scraping...\n",
      "  [82/156] Scraping...\n",
      "  [83/156] Scraping...\n",
      "  [84/156] Scraping...\n",
      "  [85/156] Scraping...\n",
      "  [86/156] Scraping...\n",
      "  [87/156] Scraping...\n",
      "  [88/156] Scraping...\n",
      "  [89/156] Scraping...\n",
      "  [90/156] Scraping...\n",
      "  [91/156] Scraping...\n",
      "  [92/156] Scraping...\n",
      "  [93/156] Scraping...\n",
      "  [94/156] Scraping...\n",
      "  [95/156] Scraping...\n",
      "  [96/156] Scraping...\n",
      "  [97/156] Scraping...\n",
      "  [98/156] Scraping...\n",
      "  [99/156] Scraping...\n",
      "  [100/156] Scraping...\n",
      "  [101/156] Scraping...\n",
      "  [102/156] Scraping...\n",
      "  [103/156] Scraping...\n",
      "  [104/156] Scraping...\n",
      "  [105/156] Scraping...\n",
      "  [106/156] Scraping...\n",
      "  [107/156] Scraping...\n",
      "  [108/156] Scraping...\n",
      "  [109/156] Scraping...\n",
      "  [110/156] Scraping...\n",
      "  [111/156] Scraping...\n",
      "  [112/156] Scraping...\n",
      "  [113/156] Scraping...\n",
      "  [114/156] Scraping...\n",
      "  [115/156] Scraping...\n",
      "  [116/156] Scraping...\n",
      "  [117/156] Scraping...\n",
      "  [118/156] Scraping...\n",
      "  [119/156] Scraping...\n",
      "  [120/156] Scraping...\n",
      "  [121/156] Scraping...\n",
      "  [122/156] Scraping...\n",
      "  [123/156] Scraping...\n",
      "  [124/156] Scraping...\n",
      "  [125/156] Scraping...\n",
      "  [126/156] Scraping...\n",
      "  [127/156] Scraping...\n",
      "  [128/156] Scraping...\n",
      "  [129/156] Scraping...\n",
      "  [130/156] Scraping...\n",
      "  [131/156] Scraping...\n",
      "  [132/156] Scraping...\n",
      "  [133/156] Scraping...\n",
      "  [134/156] Scraping...\n",
      "  [135/156] Scraping...\n",
      "  [136/156] Scraping...\n",
      "  [137/156] Scraping...\n",
      "  [138/156] Scraping...\n",
      "  [139/156] Scraping...\n",
      "  [140/156] Scraping...\n",
      "  [141/156] Scraping...\n",
      "  [142/156] Scraping...\n",
      "  [143/156] Scraping...\n",
      "  [144/156] Scraping...\n",
      "  [145/156] Scraping...\n",
      "  [146/156] Scraping...\n",
      "  [147/156] Scraping...\n",
      "  [148/156] Scraping...\n",
      "  [149/156] Scraping...\n",
      "  [150/156] Scraping...\n",
      "  [151/156] Scraping...\n",
      "  [152/156] Scraping...\n",
      "  [153/156] Scraping...\n",
      "  [154/156] Scraping...\n",
      "  [155/156] Scraping...\n",
      "  [156/156] Scraping...\n",
      "\n",
      "‚úÖ 468 recettes sauvegard√©es dans marmiton.csv\n",
      "\n",
      "üìä Statistiques:\n",
      "  - Total recettes collect√©es: 468\n",
      "  - Types: {'plat': 255, 'dessert': 162, 'entree': 51}\n",
      "  - Cuisines: {'fran√ßaise': 463, 'italienne': 5}\n",
      "  - Top 5 premi√®res recettes :\n",
      "0    Paupiettes de veau aux oignons et tomates\n",
      "1                Porc au lait de coco (Tahiti)\n",
      "2          Tarte √† la courgette et aux lardons\n",
      "3               Cuisse de canard √† l'orientale\n",
      "4                   Gratin de courge butternut\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "source": [
    "class MarmitonScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.marmiton.org\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "    def get_recipe_urls(self, category=\"plat-principal\", max_pages=3):\n",
    "        urls = []\n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                search_url = f\"{self.base_url}/recettes/recherche.aspx?aqt={category}&page={page}\"\n",
    "                response = self.session.get(search_url, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                links = soup.find_all('a', href=re.compile(r'/recettes/recette_'))\n",
    "                \n",
    "                page_urls = [urljoin(self.base_url, link.get('href')) for link in links if link.get('href')]\n",
    "                urls.extend(page_urls)\n",
    "                print(f\"‚úì Page {page} - {len(page_urls)} URLs trouv√©es\")\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur page {page}: {e}\")\n",
    "        return list(set(urls))\n",
    "\n",
    "    def extract_recipe_details(self, url):\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            title = self._extract_title(soup)\n",
    "            \n",
    "            # la classification pour les statistiques\n",
    "            recipe_data = {\n",
    "                'url': url,\n",
    "                'titre': title,\n",
    "                'type_recette': self._classify_type(title),\n",
    "                'cuisine': self._classify_cuisine(title),\n",
    "                'ingredients': str(self._extract_ingredients(soup)),\n",
    "                'etapes': str(self._extract_steps(soup)),\n",
    "                'date_scraping': datetime.now().isoformat()\n",
    "            }\n",
    "            return recipe_data\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _extract_title(self, soup):\n",
    "        title = soup.find('h1')\n",
    "        return title.text.strip() if title else \"Sans titre\"\n",
    "\n",
    "    def _classify_type(self, title):\n",
    "        title = title.lower()\n",
    "        if any(x in title for x in ['tarte', 'g√¢teau', 'mousse', 'dessert']): return 'dessert'\n",
    "        if any(x in title for x in ['entr√©e', 'salade', 'soupe']): return 'entree'\n",
    "        return 'plat'\n",
    "\n",
    "    def _classify_cuisine(self, title):\n",
    "        title = title.lower()\n",
    "        if 'pizza' in title or 'pasta' in title: return 'italienne'\n",
    "        return 'fran√ßaise'\n",
    "\n",
    "    def _extract_ingredients(self, soup):\n",
    "        names = soup.select('span[class*=\"IngredientName\"]')\n",
    "        qtys = soup.select('span[class*=\"IngredientQuantity\"]')\n",
    "        return [f\"{q.text.strip()} {n.text.strip()}\" for n, q in zip(names, qtys)]\n",
    "\n",
    "    def _extract_steps(self, soup):\n",
    "        items = soup.select('div[class*=\"RecipeStep\"]')\n",
    "        return [item.get_text().strip() for item in items]\n",
    "\n",
    "    def scrape_all(self, categories=['plat-principal'], max_pages=1):\n",
    "        all_results = []\n",
    "        for cat in categories:\n",
    "            print(f\"\\nüìÇ Cat√©gorie : {cat}\")\n",
    "            urls = self.get_recipe_urls(cat, max_pages)\n",
    "            for i, url in enumerate(urls, 1):\n",
    "                print(f\"  [{i}/{len(urls)}] Scraping...\")\n",
    "                data = self.extract_recipe_details(url)\n",
    "                if data: all_results.append(data)\n",
    "                time.sleep(random.uniform(0.5, 1.5))\n",
    "        return all_results\n",
    "\n",
    "    def save_to_csv(self, recipes, filename='marmiton.csv'):\n",
    "        if not recipes:\n",
    "            print(\"Aucune donn√©e √† sauvegarder.\")\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame(recipes)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n‚úÖ {len(df)} recettes sauvegard√©es dans {filename}\")\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = MarmitonScraper()\n",
    "    \n",
    "    print(\"üöÄ D√©marrage du scraping Marmiton...\")\n",
    "    \n",
    "    # 1. Lancer le scraping\n",
    "    recipes = scraper.scrape_all(\n",
    "        categories=['plat-principal', 'entree', 'dessert'],\n",
    "        max_pages=10  \n",
    "    )\n",
    "    \n",
    "    # 2. Sauvegarder ET r√©cup√©rer le DataFrame pour les stats\n",
    "    df = scraper.save_to_csv(recipes)\n",
    "    \n",
    "    # 3. Afficher les statistiques uniquement si on a des donn√©es\n",
    "    if not df.empty:\n",
    "        print(f\"\\nüìä Statistiques:\")\n",
    "        print(f\"  - Total recettes collect√©es: {len(recipes)}\")\n",
    "        \n",
    "        \n",
    "        if 'type_recette' in df.columns:\n",
    "            print(f\"  - Types: {df['type_recette'].value_counts().to_dict()}\")\n",
    "        if 'cuisine' in df.columns:\n",
    "            print(f\"  - Cuisines: {df['cuisine'].value_counts().to_dict()}\")\n",
    "        \n",
    "        print(f\"  - Top 5 premi√®res recettes :\\n{df['titre'].head()}\")\n",
    "    else:\n",
    "        print(\"‚ö† Aucune donn√©e n'a pu √™tre extraite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4767e-e178-4c0c-8864-11f976d987d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d5d96-25c6-43b7-aa2a-6e5e5a90feea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
